<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zidong Cao</title>

  <meta name="author" content="Zidong Cao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/hkust-logo.png">

  <script type="text/javascript">

    function display(id) {
      var traget = document.getElementById(id);
      if (traget.style.display == "none") {
        traget.style.display = "";
      } else {
        traget.style.display = "none";
      }
    }  
  </script>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zidong Cao</name>
              </p>
              <p> I am a third-year PhD student at 
                <a href="https://www.hkust-gz.edu.cn/">HKUST (GZ campus)</a>, fortunately supervised by Prof. <a href="https://www.hkust-gz.edu.cn/people/hui-xiong/">Hui Xiong</a>. 
              </p>
              <p> I received my Bachelor and Master degree from <a href="https://www.xjtu.edu.cn/">Xi'an Jiaotong University</a> in 2019 and 2022 respectively, fortunately supervised by Prof. <a href="https://dblp.org/pid/26/455.html">Zejian Yuan</a> and Dr. <a href="https://github.com/anglixjtu"> Ang Li</a>.
              </p>
              <p> My previous research focuses on vision foundation model, and 360 vision. Recently, I focus on generative recommendation model (GRM). Welcome to have collaboration with me!
              </p>
              <p style="text-align:center">
                <a href="caozidong1996@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q1FcZzIAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/caozidong">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:80%;max-width:80%" alt="profile photo" src="images/profile.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p> <strong>2025-09:</strong> <a href="https://openreview.net/pdf?id=d0bm4upnQ8">ST2360D</a> is accepted by NeurIPS 2025. </p>
            <p> <strong>2025-03:</strong> <a href="https://arxiv.org/abs/2406.13378">PanDA</a> is accepted by CVPR 2025. </p>
            <p> <strong>2024-09:</strong> <a href="https://ieeexplore.ieee.org/document/10803042">CRF360D</a> is accepted by RAL & ICRA 2025. </p>
            <p> <strong>2024-06:</strong> <a href="https://arxiv.org/abs/2304.07967">DESR</a> is accepted by TAI 2024. </p>
            <p> <strong>2023-07:</strong> <a href="https://arxiv.org/abs/2308.08114">OmniZoomer</a> is accepted by ICCV 2023. </p>
            </div>
            
          </td>
        </tr>
      
        
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Preprint</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/arxiv_360.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2405.00351">
              <papertitle>Learning High-Quality Navigation and Zooming on Omnidirectional Images in Virtual Reality</papertitle>
            </a>
            <br>
            <strong>Zidong Cao</strong>, Zhan Wang, Yexin Liu, Yan-Pei Cao, Ying Shan, Wei Zeng, Lin Wang
            <br>
            <em>arXiv preprint</em>, 2024
            <br>
            <a href="https://arxiv.org/pdf/2405.00351">Paper</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/himap.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2401.03203">
              <papertitle>Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity Monocular Dense Mapping</papertitle>
            </a>
            <br>
            Tongyan Hua, Haotian Bai, <strong>Zidong Cao</strong>, Ming Liu, Dacheng Tao, Lin Wang
            <br>
            <em>arXiv preprint</em>, 2024
            <br>
            <a href="https://arxiv.org/pdf/2401.03203">Paper</a>
          </td>
        </tr>

      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>First Author Publications</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/nips.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://openreview.net/pdf?id=d0bm4upnQ8">
              <papertitle>ST2360D: Spatial-to-Temporal Consistency for Training-free 360 Monocular Depth Estimation</papertitle>
            </a>
            <br>
            <strong>Zidong Cao*</strong>, Jinjing Zhu*, Hao Ai*, Lutao Jiang, Yuanhuiyi Lyu, Hui Xiong
            <br>
            <em>The Thirty-Ninth Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2025
            <br>
            <a href="https://openreview.net/pdf?id=d0bm4upnQ8">Paper</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/panda.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2406.13378">
              <papertitle>PanDA: Towards Panoramic Depth Anything with Unlabeled Panoramas and Mobius Spatial Augmentation</papertitle>
            </a>
            <br>
            <strong>Zidong Cao</strong>, Jinjing Zhu, Weiming Zhang, Hao Ai, Haotian Bai, Hengshuang Zhao, Lin Wang
            <br>
            <em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2406.13378">Paper</a> /
            <a href="https://github.com/caozidong/PanDA">Code</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/crf360d.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://ieeexplore.ieee.org/document/10803042">
              <papertitle>Monocular 360 Depth Estimation via Spherical Fully-Connected CRFs</papertitle>
            </a>
            <br>
            <strong>Zidong Cao</strong>, Lin Wang
            <br>
            <em>IEEE Robotics and Automation Letters (<b>RAL</b>), IEEE International Conference on Robotics & Automation <b>(ICRA)</b></em>, 2025
            <br>
            <a href="https://ieeexplore.ieee.org/document/10803042">Paper</a> /
            <a href="https://github.com/caozidong/CRF360D">Code</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/desr.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2304.07967">
              <papertitle>360 High-Resolution Depth Estimation via Uncertainty-aware Structural Knowledge Transfer</papertitle>
            </a>
            <br>
            <strong>Zidong Cao</strong>, Hao Ai, Athanasios V. Vasilakos, Lin Wang
            <br>
            <em>IEEE Transactions on Artificial Intelligence (<b>TAI</b>)</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2304.07967">Paper</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/omnizoomer.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2308.08114">
              <papertitle>OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution</papertitle>
            </a>
            <br>
            <strong>Zidong Cao</strong>, Hao Ai, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Lin Wang
            <br>
            <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2308.08114">Paper</a> /
            <a href="https://github.com/caozidong/OmniZoomer">Code</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/bmvc.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://www.bmva-archive.org.uk/bmvc/2021/assets/papers/1436.pdf">
              <papertitle>Surround-view Free Space Boundary Detection with Polar Representation</papertitle>
            </a>
            <br>
            <strong>Zidong Cao</strong>, Ang Li, Zhiliang Xiong, Zejian Yuan
            <br>
            <em>British Machine Vision Conference (<b>BMVC</b>)</em>, 2021
            <br>
            <a href="https://www.bmva-archive.org.uk/bmvc/2021/assets/papers/1436.pdf">Paper</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/icip.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://ieeexplore.ieee.org/abstract/document/9506137/">
              <papertitle>Self-Supervised Depth Completion via Adaptive Sampling and Relative Consistency</papertitle>
            </a>
            <br>
            <strong>Zidong Cao</strong>, Ang Li, Zejian Yuan
            <br>
            <em>IEEE International Conference on Image Processing (<b>ICIP</b>)</em>, 2021
            <br>
            <a href="https://ieeexplore.ieee.org/abstract/document/9506137/">Paper</a> /
            <a href="https://github.com/caozidong/Depth-Completion">Code</a>
          </td>
        </tr>

      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Other Publications</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/ijcv.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2502.10444?">
              <papertitle>A Survey of Representation Learning, Optimization Strategies, and Applications for Omnidirectional Vision</papertitle>
            </a>
            <br>
            Hao Ai, <strong>Zidong Cao</strong>, Lin Wang
            <br>
            <em>International Journal of Computer Vision (<b>IJCV</b>)</em>, 2025
            <br>
            <a href="https://arxiv.org/pdf/2502.10444?">Paper</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/iccv_25.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_Depth_Any_Event_Stream_Enhancing_Event-based_Monocular_Depth_Estimation_via_ICCV_2025_paper.pdf">
              <papertitle>Depth Any Event Stream: Enhancing Event-based Monocular Depth Estimation via Dense-to-Sparse Distillation</papertitle>
            </a>
            <br>
            Jinjing Zhu, Tianbo Pan, <strong>Zidong Cao</strong>, Yexin Liu, James T. Kwok, Hui Xiong
            <br>
            <em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2025
            <br>
            <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zhu_Depth_Any_Event_Stream_Enhancing_Event-based_Monocular_Depth_Estimation_via_ICCV_2025_paper.pdf">Paper</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/tvcg.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2401.10564">
              <papertitle>Dream360: Diverse and Immersive Outdoor Virtual Scene Creation via Transformer-Based 360Â° Image Outpainting</papertitle>
            </a>
            <br>
            Hao Ai, <strong>Zidong Cao</strong>, Haonan Lu, Chen Chen, Jian Ma, Pengyuan Zhou, Tae-Kyun Kim, Pan Hui, Lin Wang
            <br>
            <em>IEEE Transactions on Visualization and Computer Graphics (<b>TVCG</b>)</em>, 2024
            <br>
            <a href="https://arxiv.org/pdf/2401.10564">Paper</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/icra_event.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/pdf/2309.12842">
              <papertitle>Srfnet: Monocular Depth Estimation with Fine-grained Structure via Spatial Reliability-oriented Fusion of Frames and Events</papertitle>
            </a>
            <br>
            Tianbo Pan, <strong>Zidong Cao</strong>, Lin Wang
            <br>
            <em>IEEE International Conference on Robotics and Automation (<b>ICRA</b>)</em>, 2024
            <br>
            <a href="https://arxiv.org/pdf/2309.12842">Paper</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/uda.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Both_Style_and_Distortion_Matter_Dual-Path_Unsupervised_Domain_Adaptation_for_CVPR_2023_paper.pdf">
              <papertitle>Both Style and Distortion Matter: Dual-Path Unsupervised Domain Adaptation for Panoramic Semantic Segmentation</papertitle>
            </a>
            <br>
            Xu Zheng, Jinjing Zhu, Yexin Liu, <strong>Zidong Cao</strong>, Chong Fu, Lin Wang
            <br>
            <em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Both_Style_and_Distortion_Matter_Dual-Path_Unsupervised_Domain_Adaptation_for_CVPR_2023_paper.pdf">Paper</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/hrdfuse.png' alt="paper" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ai_HRDFuse_Monocular_360deg_Depth_Estimation_by_Collaboratively_Learning_Holistic-With-Regional_Depth_CVPR_2023_paper.pdf">
              <papertitle>HRDFuse: Monocular 360deg Depth Estimation by Collaboratively Learning Holistic-With-Regional Depth Distributions</papertitle>
            </a>
            <br>
            Hao Ai, <strong>Zidong Cao</strong>, Yan-Pei Cao, Ying Shan, Lin Wang
            <br>
            <em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ai_HRDFuse_Monocular_360deg_Depth_Estimation_by_Collaboratively_Learning_Holistic-With-Regional_Depth_CVPR_2023_paper.pdf">Paper</a>
          </td>
        </tr>

      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              Conference reviewer: CVPR 2024, ICRA 2024, CVPR 2025, CVPR 2026.
            </p>

            <p>
              Journal reviewer: TIP, TMM, RAL.
            </p>
            
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
              <br>
              <a href="https://jonbarron.info/">Website Template</a>
            </p>
          </td>
        </tr>
      </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
